{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_log_mel_spectrogram(file_path, n_mels=128, duration=5.0):\n",
    "    # Load the audio file\n",
    "    waveform, sr = librosa.load(file_path, duration=duration)\n",
    "    if len(waveform) == 0:\n",
    "            # Skip empty audio files\n",
    "            return None\n",
    "    \n",
    "    hop_length = int(sr * (0.02 - 0.01))\n",
    "    \n",
    "    # Extract the Log-Mel-spectrogram\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(waveform, sr=sr, n_mels=n_mels, hop_length=hop_length)\n",
    "    log_mel_spectrogram = librosa.amplitude_to_db(mel_spectrogram, ref=np.max)\n",
    "    \n",
    "    return log_mel_spectrogram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wake_word_dir = './wake_word/'\n",
    "negative_samples_dir = './neg_samples/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "wake_word_files = os.listdir(wake_word_dir)\n",
    "for file in wake_word_files:\n",
    "    file_path = os.path.join(wake_word_dir, file)\n",
    "    feature = preprocess_log_mel_spectrogram(file_path)\n",
    "    features.append(feature)\n",
    "    labels.append(1)  # 1 represents the wake word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "negative_samples_files = os.listdir(negative_samples_dir)\n",
    "for file in negative_samples_files:\n",
    "    file_path = os.path.join(negative_samples_dir, file)\n",
    "    feature = preprocess_log_mel_spectrogram(file_path)\n",
    "    features.append(feature)\n",
    "    labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_shape = max(feature.shape for feature in features)\n",
    "\n",
    "padded_features = []\n",
    "for feature in features:\n",
    "    pad_width = [(0, max_shape[0] - feature.shape[0]), (0, max_shape[1] - feature.shape[1])]\n",
    "    padded_feature = np.pad(feature, pad_width, mode='constant')\n",
    "    padded_features.append(padded_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(padded_features)\n",
    "y = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "#két részre osztom az dathalmaz, az *_trainval-ba mennek a tanító+valikdációs adatok, a *_test-be a tesztadatok\n",
    "x_trainval, x_test, y_trainval, y_test = train_test_split(x, y, test_size=0.15, random_state=42)\n",
    "#x_train, x_valid, y_train, y_valid, x_test,y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#ezt követően a tanító + validációs adatokat is két részrte osztom ezzel megkapva a 3 datasetet\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_trainval, y_trainval, test_size=0.1765, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.reshape(x_train, (*x_train.shape, 1))\n",
    "x_test = np.reshape(x_test, (*x_test.shape, 1))\n",
    "x_valid = np.reshape(x_valid, (*x_valid.shape, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Conv2D(32, (3, 3), activation='relu', input_shape=x_train[0].shape))\n",
    "model1.add(MaxPooling2D((2, 2)))\n",
    "model1.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model1.add(MaxPooling2D((2, 2)))\n",
    "model1.add(Flatten())\n",
    "model1.add(Dense(64, activation='relu'))\n",
    "model1.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(optimizer='adam',\n",
    "              #optimizer='sgd',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint = ModelCheckpoint('wake_wordv1.h5', monitor='val_loss', save_best_only=True)\n",
    "es = EarlyStopping(monitor='val_loss', min_delta=0, patience = 5)\n",
    "\n",
    "callbacks = [checkpoint, es]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "hist1 = model1.fit(x_train, y_train,\n",
    "                 epochs=1000,\n",
    "                 batch_size=batch_size,\n",
    "                 validation_data=(x_valid, y_valid),\n",
    "                 callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss = hist1.history['loss']\n",
    "val_loss=hist1.history['val_loss']\n",
    "acc = hist1.history['accuracy']\n",
    "val_acc = hist1.history['val_accuracy']\n",
    "\n",
    "epochs = range(len(loss))\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'g', label='Validation loss')\n",
    "plt.plot(epochs, acc, 'y', label='Accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss,test_acc = model1.evaluate(x_test)\n",
    "print('Training Acc: ',acc[-1])\n",
    "print('Validation Acc: ', val_acc[-1])\n",
    "print('Test Loss: ',test_loss)\n",
    "print('Test Acc: ',test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
